{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "cell_execution_strategy": "setup",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/DimitriBolt/Social-processes/blob/main/StochasticModellingOfSocialProcesses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mini Research: Applicability (or Inapplicability) of Basic Statistical Approaches for Describing Social Processes\n",
    "=======\n",
    "<p align='right'><i>\"All science is either well-known physics or deep research the real world.\"</i>  \n",
    "— Echo of Ernest Rutherford</p>\n",
    "\n",
    "---\n",
    "\n",
    "# 0. Research Goal\n",
    "To prove or disprove that the characteristics of economic agents are random variables and cannot be effectively modeled or predicted.\n",
    "\n",
    "In the words of the Math:\n",
    "Dimitri Bolt aims to determine whether social processes constitute sets to which the framework of modern probability theory can be applied. Specifically, it remains unclear whether these processes satisfy the **Kolmogorov axioms**. By engaging in mathematical statistics, Dimitri Bolt seeks to draw conclusions about the applicability of Kolmogorov’s axiomatic system based on observed samples drawn from such sets.\n",
    "\n",
    "<font color='yellow'>*TODO*</font>: use this https://en.wikipedia.org/wiki/Empirical_distribution_function\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Experimental Data  \n",
    "I developed software that collects data about economic agents in under a minute. For the purposes of this research, I propose focusing on a dataset of **10,000 American companies**, as presented in formats like this Yahoo Finance page:  \n",
    "https://finance.yahoo.com/quote/MSFT/\n",
    "\n",
    "Approximately **95% of the effort** in this research was spent developing this data collection tool. It's worth noting that companies like Bloomberg offer similar data, but real-time subscriptions cost around **$50,000/month**, which is not feasible for academic research.\n",
    "\n",
    "From a mathematical point of view, the market can be considered as an object:\n",
    "$$\n",
    "\\vec{M}(t) = \\begin{bmatrix}\n",
    "p_1(t) \\\\\n",
    "p_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "p_n(t)\n",
    "\\end{bmatrix}, \\quad n \\approx 10,000\n",
    "$$\n",
    "\n",
    "where each function $p_i(t)$ represents the time series (e.g., price, profit) of an economic agent.\n",
    "\n",
    "This implies $\\vec{M}(t) \\in \\mathbb{R}^{10,000}$.\n",
    "\n",
    "As a **zero-order assumption**, we treat all economic agents as independent—i.e., all dimensions are equally important and unrelated."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Techcal section for connectig with main data collection software.\n",
    "\n",
    "Shouls be hide."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Techical code chank for connection with main data collection software.\n",
    "!pip install oracledb\n",
    "import oracledb\n",
    "from pandas import DataFrame\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm  # Using statsmodels for efficient ACF calculation\n",
    "from google.colab import drive;\n",
    "# drive.mount ('/content/drive');\n",
    "# !ls -list /content/sample_data/admin/\n",
    "\n",
    "connection = oracledb.connect(\n",
    "        # config_dir=\"/content/drive/MyDrive/Statistics/admin/\",\n",
    "        config_dir=\"/content/sample_data/admin/\",\n",
    "        user=\"ADMIN\",\n",
    "        password=\"Db123456!@#$%^\",\n",
    "        dsn=\"yahooprices_high\",\n",
    "        # wallet_location=\"/content/drive/MyDrive/Statistics/admin/\",\n",
    "        wallet_location=\"/content/sample_data/admin/\",\n",
    "        wallet_password=\"Db123456!\")\n",
    "\n",
    "# The example for 9 of 10,000 tikers.\n",
    "with connection.cursor() as cursor:\n",
    "  cursor.execute(\"\"\"\n",
    "  SELECT TRUNC(date'1970-01-01' + (timestamp / 86400)) as time,\n",
    "    symbol,\n",
    "    ADJCLOSE\n",
    "  FROM Commodities\n",
    "  WHERE date'1970-01-01'+ (timestamp / 86400) < CURRENT_DATE - 1\n",
    "  \"\"\")\n",
    "  col_names = [c.name for c in cursor.description]\n",
    "  data: list = cursor.fetchall()\n",
    "df: DataFrame = pandas.DataFrame(data, columns=col_names)\n",
    "# df = df.dropna()\n",
    "indeces: DataFrame = df.pivot_table(values='ADJCLOSE', index='TIME', columns='SYMBOL', aggfunc='mean')\n",
    "\n",
    "# Stock prices of telecommunication companies.\n",
    "with connection.cursor() as cursor:\n",
    "  cursor.execute(\"\"\"\n",
    "  SELECT TRUNC(date'1970-01-01' + (timestamp / 86400)) as time,\n",
    "    symbol,\n",
    "    ADJCLOSE\n",
    "  FROM Technology\n",
    "  WHERE date'1970-01-01'+ (timestamp / 86400) < CURRENT_DATE - 1\n",
    "  \"\"\")\n",
    "  col_names = [c.name for c in cursor.description]\n",
    "  data: list = cursor.fetchall()\n",
    "df: DataFrame = pandas.DataFrame(data, columns=col_names)\n",
    "# df = df.dropna()\n",
    "tech: DataFrame = df.pivot_table(values='ADJCLOSE', index='TIME', columns='SYMBOL', aggfunc='mean')\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualization\n",
    "Below in this section are examples of the format and visualizations of the analyzed data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title\n",
    "display(tech)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# @title\n",
    "df = tech.copy()\n",
    "selected_columns = [\"AAPL\", \"ADBE\", \"AMD\", \"ADSK\", \"MSFT\", \"NVDA\"]\n",
    "# df = df[selected_columns]\n",
    "plt.figure(figsize=(20, 10))                  # Adjust figure size as needed\n",
    "\n",
    "\n",
    "# Get the number of columns (excluding 'TIME') for subplot layout\n",
    "num_plots = len(df[selected_columns].columns)\n",
    "num_rows = int(np.ceil(num_plots / 2))        # Calculate rows for 2 columns per row\n",
    "\n",
    "# Loop through columns\n",
    "for i, column in enumerate(df[selected_columns].columns):\n",
    "    plt.subplot(num_rows, 2, i + 1)           # Create subplot in the grid\n",
    "    plt.plot(df.index, df[column])            # Plot the data using df.index for x-axis\n",
    "    plt.title(column)                         # Set subplot title to column name\n",
    "    plt.xticks(rotation=45, ha='right')       # Rotate x-axis labels for readability\n",
    "\n",
    "plt.tight_layout()                            # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Detrending the Data  \n",
    "As input data, may be to use:  \n",
    "$$\n",
    "M_i(t)_{detrended} = p_i(t) - T(t)_{trend}\n",
    "$$\n",
    "where $T(t)$ is a common trend.  \n",
    "It is generally agreed that the *Dow Jones Industrial Average* can be used as $T(t)$."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalize all columns ftom\n",
    "df_normalized = df.copy()\n",
    "for column in df.columns:\n",
    "    df_normalized[column] = (df[column] - df[column].min()) / (df[column].max() - df[column].min())\n",
    "\n",
    "# Normalize trend.\n",
    "trend = (indeces['^DJI'] - indeces['^DJI'].min()) / (indeces['^DJI'].max() - indeces['^DJI'].min())\n",
    "\n",
    "# Create the detrended DataFrame.\n",
    "df_detrending = df_normalized.copy()\n",
    "for column in df_normalized.columns:\n",
    "    df_detrending[column] = df_normalized[column] - trend\n",
    "\n",
    "# Chart all columns of the detrended DataFrame\n",
    "plt.figure(figsize=(20, 10))\n",
    "num_plots = len(df_detrending[selected_columns].columns) - 1\n",
    "num_rows = int(np.ceil(num_plots / 2))\n",
    "\n",
    "import matplotlib.ticker as mtick           # Import for percentage formatting\n",
    "for i, column in enumerate(df_detrending[selected_columns].columns):\n",
    "  if True :\n",
    "    plt.subplot(num_rows, 2, i + 1)\n",
    "    plt.plot(df_detrending.index, df_detrending[column])\n",
    "    plt.title(f\"Detrended {column}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))  # Format y-axis as percentage\n",
    "\n",
    "    # Add horizontal grid lines every 10%\n",
    "    plt.grid(axis='y', which='major', linestyle='--', color='gray')\n",
    "    plt.gca().yaxis.set_major_locator(mtick.MultipleLocator(0.1))  # Set major ticks every 10%\n",
    "\n",
    "    # Add main horizontal line at y=0\n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Also, the  *Standard and Poor's 500* can be used as $T(t)$."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let normalizes trend be a Standard and Poor's 500.\n",
    "trend = (indeces['^GSPC'] - indeces['^GSPC'].min()) / (indeces['^GSPC'].max() - indeces['^GSPC'].min())\n",
    "\n",
    "# Create the detrended DataFrame\n",
    "df_detrending = df_normalized.copy()\n",
    "for column in df_normalized.columns:\n",
    "    df_detrending[column] = df_normalized[column] - trend\n",
    "\n",
    "# Chart all columns of the detrended DataFrame\n",
    "plt.figure(figsize=(20, 10))\n",
    "num_plots = len(df_detrending[selected_columns].columns) - 1\n",
    "num_rows = int(np.ceil(num_plots / 2))\n",
    "\n",
    "import matplotlib.ticker as mtick                                     # Import for percentage formatting\n",
    "for i, column in enumerate(df_detrending[selected_columns].columns):\n",
    "  if True:\n",
    "    plt.subplot(num_rows, 2, i + 1)\n",
    "    plt.plot(df_detrending.index, df_detrending[column])\n",
    "    plt.title(f\"Detrended {column}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))  # Format y-axis as percentage\n",
    "\n",
    "    # Add horizontal grid lines every 10%\n",
    "    plt.grid(axis='y', which='major', linestyle='--', color='gray')\n",
    "    plt.gca().yaxis.set_major_locator(mtick.MultipleLocator(0.1))     # Set major ticks every 10%\n",
    "\n",
    "    # Add main horizontal line at y=0\n",
    "    plt.axhline(y=0, color='black', linestyle='-')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2.  Calculating the Discrete Autocorrelation Function `G(k)` for a Time Series.\n",
    "\n",
    "The first idea is to examine the **autocorrelation function** of the time series.\n",
    "\n",
    "## 2.1  This function calculates the discrete analogue of the continuous autocorrelation function\n",
    "\n",
    "$$G(τ) = \\lim_{T\\to\\infty}\\frac{1}{T}\\int_{0}^{T} f(t-\\tau)*f(t) \\,dt$$\n",
    "using the formula:\n",
    "\n",
    "$$G(k) = \\frac{1}{N} * \\sum_{n=k}^{N-1} f(n-k)* f(n)$$\n",
    "\n",
    "where $k$ is the lag in time steps."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_autocorrelation_G(df: pd.DataFrame,\n",
    "                                value_column: str = \"MSFT\",\n",
    "                                time_column: str = \"TIME\",\n",
    "                                max_lag: int = None,\n",
    "                                normalization: str = \"biased\") -> pd.Series:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        df (pd.DataFrame):        DataFrame containing the time series data. Must have columns specified by value_column and time_column.\n",
    "        value_column (str):       The name of the column containing the time series values (f(t)). Defaults to \"MSFT\".\n",
    "        time_column (str):        The name of the column containing the time stamps (t). Defaults to \"TIME\". Although not directly used in the standard statsmodels ACF calculation (which assumes sequential order), it's kept for consistency with the problem description.\n",
    "        max_lag (int, optional):  The maximum lag (k) to compute the autocorrelation for. If None, it defaults to N-1, where N is the number of data points. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A pandas Series containing the autocorrelation values G(k), indexed by the lag k (from 0 to max_lag).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    f_t = df[value_column].dropna()           # Extract the time series values from the DataFrame\n",
    "    N = len(f_t)\n",
    "\n",
    "    # Determine the number of lags for the ACF calculation\n",
    "    if max_lag is None:\n",
    "        nlags = N - 1\n",
    "    else:\n",
    "        nlags = min(max_lag, N - 1)  # Ensure max_lag doesn't exceed N-1\n",
    "\n",
    "    f_values = f_t.values                     # Use numpy array for efficiency\n",
    "    autocorr_G = np.zeros(nlags + 1)          # Array to store results for lags 0 to nlags\n",
    "\n",
    "    for k in range(nlags + 1):\n",
    "      # Sum f(n) * f(n-k) for n from k to N-1\n",
    "      # f[k:] corresponds to f(n) for n=k...N-1\n",
    "      # f[:N-k] corresponds to f(n-k) for n=k...N-1\n",
    "      product_sum = np.sum(f_values[k:] * f_values[:N - k])\n",
    "      if normalization == \"unbiased\":\n",
    "        autocorr_G[k] = product_sum / (N - k)\n",
    "      else:   # Default to \"biased\"\n",
    "        autocorr_G[k] = product_sum / N\n",
    "\n",
    "    # Create a pandas Series for the result\n",
    "    lags = np.arange(nlags + 1)\n",
    "    result_series = pd.Series(autocorr_G, index=lags, name=\"Autocorrelation_G\")\n",
    "    result_series.index.name = \"Lag_k\"\n",
    "\n",
    "    return result_series"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Calculating Autocorrelation G(k) for all possible lags (N-1):\n",
    "- The normalization $\\frac{1}{N}$ replaces $\\frac{1}{T}$. This is a common definition (sometimes called the \"biased\" estimator)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Example with full lags ---\n",
    "autocorr_full = calculate_autocorrelation_G(df=df_detrending, value_column=\"AAPL\", normalization=\"biased\")  # max_lag=None by default, changed value_column\n",
    "# Plot the results\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.stem(autocorr_full.index, autocorr_full.values, basefmt=\" \")\n",
    "plt.xlabel(\"Lag (k time steps)\")\n",
    "plt.ylabel(\"G(k)\")\n",
    "plt.title(r'Discrete Autocorrelation G(k). The \"biased\" normalization $\\frac{1}{N}$ replaces $\\frac{1}{T}$.')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2  About the  implemention the discrete equivalent of the autocorrelation function $G(τ)$.\n",
    "\n",
    "The formula $G(τ) = \\lim_{T\\to\\infty}\\frac{1}{T}\\int_{0}^{T} f(t-\\tau)*f(t) \\,dt$ is the definition for a continuous-time autocorrelation function.\n",
    "\n",
    "Since my $f(t)$ is a discrete function (values from `df_detrending[\"AAPL\"]` at specific times `df.index`), we need to use the discrete-time autocorrelation function. Assuming the time steps are reasonably uniform, the discrete equivalent is typically calculated as:\n",
    "\n",
    "\n",
    "$$G(k) = \\frac{1}{N} * \\sum_{n=k}^{N-1} f(n-k)* f(n)$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $N$ is the total number of data points (length of the time series).\n",
    "- $k$ is the lag (the discrete equivalent of $\\tau$), representing the number of time steps shifted. $k$ ranges from 0 up to a chosen maximum lag (often $N-1$ or less).\n",
    "- $f(n)$ is the value of the series at index $n$ (corresponding to `df[\"MSFT\"].iloc[n]`).\n",
    "- $f(n-k)$ is the value of the series $k$ time steps before $n$.\n",
    "- The sum $\\sum$ replaces the integral $∫$.\n",
    "- The normalization $\\frac{1}{N}$ replaces $\\frac{1}{T}$. This is a common definition (sometimes called the \"biased\" estimator). Another definition uses $\\frac{1}{N - k}$ for normalization (the \"unbiased\" estimator). We will use the $\\frac{1}{N}$ normalization, which aligns more closely with provided continuous formula's $\\frac{1}{T}$ factor.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "------------------------"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Calculating Autocorrelation G(k) for all possible lags (N-1):\n",
    "- Another definition uses $\\frac{1}{N - k}$ for normalization (the \"unbiased\" estimator)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Example with full lags ---\n",
    "autocorr_full = calculate_autocorrelation_G(df=df_detrending, value_column=\"AAPL\", normalization=\"unbiased\")  # max_lag=None by default, changed value_column\n",
    "# Plot the results\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.stem(autocorr_full.index, autocorr_full.values, basefmt=\" \")\n",
    "plt.xlabel(\"Lag (k time steps)\")\n",
    "plt.ylabel(\"G(k)\")\n",
    "plt.title(r'Discrete Autocorrelation G(k). The \"unbiased\" estimator $\\frac{1}{N - k}$ for normalization')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Using a Ready-Made Autocorrelation Function  \n",
    "It maybe useful to use a standard library implementation of the autocorrelation function.  \n",
    "**TODO:** Use `statsmodels.tsa.stattools.acf` from the `statsmodels` package."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate autocorrelation using statsmodels\n",
    "acf_values = sm.tsa.stattools.acf(df_detrending[\"AAPL\"], nlags=len(df) - 1, fft=True, missing='conservative')\n",
    "\n",
    "# Plot the autocorrelation\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.stem(range(len(acf_values)), acf_values, basefmt=\" \")\n",
    "plt.xlabel(\"Lag (k time steps)\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.title(\"Autocorrelation for 'AAPL' using ready-made autocorrelation gunction from statsmodels©\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The difference in results between `calculate_autocorrelation_G` and `sm.tsa.stattools.acf` arises from the way the autocorrelation is calculated and normalized in each method:\n",
    "\n",
    "\n",
    "1. `calculate_autocorrelation_G`:\n",
    "\n",
    "\n",
    "- This function implements a manual calculation of the autocorrelation function `G(k)` based on the formula: $G(k) = \\frac{1}{N} \\sum_{n=k}^{N-1} f(n) \\cdot f(n-k)$\n",
    "- It does not subtract the **mean** of the time series before calculating the autocorrelation. This means the raw values of the series are used directly, which can lead to different results if the series has a non-zero mean.\n",
    "- The normalization is done by dividing by $N$ (the total number of data points), which is consistent with the formula provided in the function's docstring.\n",
    "\n",
    "2.   `sm.tsa.stattools.acf`:\n",
    "\n",
    "\n",
    "- This function calculates the **autocorrelation coefficient**, which is normalized by the variance of the series. It subtracts the mean of the series before performing the calculation.\n",
    "- The formula used is: $\\text{ACF}(k) = \\frac{\\sum_{n=k}^{N-1} (f(n) - \\bar{f}) \\cdot (f(n-k) - \\bar{f})}{\\sum_{n=0}^{N-1} (f(n) - \\bar{f})^2}$ where $\\bar{f}$ is the mean of the series.\n",
    "- This normalization ensures that the autocorrelation values are bounded between -1 and 1.\n",
    "\n",
    "## Key Differences:\n",
    "- **Mean subtraction**: `sm.tsa.stattools.acf` subtracts the mean, while `calculate_autocorrelation_G` does not.\n",
    "- **Normalization**: `sm.tsa.stattools.acf` normalizes by the variance, while `calculate_autocorrelation_G` normalizes by $N$.\n",
    "- **Interpretation**: The results from `calculate_autocorrelation_G` represent the raw autocorrelation values $G(k)$, while `sm.tsa.stattools.acf` provides the autocorrelation coefficients, which are standardized.\n",
    "\n",
    "## Impact:\n",
    "If the time series has a non-zero mean or significant variance, the results from the two methods will differ. The `sm.tsa.stattools.acf` method is more commonly used in statistical analysis because it provides a standardized measure of autocorrelation.\n",
    "\n",
    "[comment]: https://wiki.loginom.ru/articles/acf.html\n",
    "-----------"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "autocorr_full = calculate_autocorrelation_G(df=df_detrending, value_column=\"AAPL\", normalization=\"biased\")  # max_lag=None by default, changed value_column\n",
    "# autocorr_full = pandas.Series(data=acf_values, index=range(len(acf_values)))\n",
    "\n",
    "# Find the indices for local maximum and minimum\n",
    "local_maximum_index_1 = np.argmax(autocorr_full[150:170]) + 150  # Adjust the range as needed\n",
    "local_maximum_index_2 = np.argmax(autocorr_full[280:320]) + 280  # Adjust the range as needed\n",
    "\n",
    "local_minimum_index_1 = np.argmin(autocorr_full[240:260]) + 240  # Adjust the range as needed\n",
    "local_minimum_index_2 = np.argmin(autocorr_full[370:390]) + 370 # Adjust the range as needed\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.stem(autocorr_full.index, autocorr_full.values, basefmt=\" \")\n",
    "\n",
    "# Plot the linear trend\n",
    "x = np.arange(0, 91)\n",
    "y = np.linspace(start=autocorr_full[0], stop=autocorr_full[91-1], num=91)\n",
    "plt.plot(x,y, color='red', linewidth=8)\n",
    "\n",
    "\n",
    "# Highlight the points\n",
    "plt.plot(local_maximum_index_1, autocorr_full[local_maximum_index_1], 'ro', markersize=16, label=f\"Local Maximum near index {local_maximum_index_1}\")\n",
    "plt.plot(local_maximum_index_2, autocorr_full[local_maximum_index_2], 'ro', markersize=16, label=f\"Local Maximum near index {local_maximum_index_2}\")\n",
    "\n",
    "plt.plot(local_minimum_index_1, autocorr_full[local_minimum_index_1], 'go', markersize=8, label=f\"Local Minimum near index {local_minimum_index_1}\")\n",
    "plt.plot(local_minimum_index_2, autocorr_full[local_minimum_index_2], 'go', markersize=8, label=f\"Local Minimum near index {local_minimum_index_2}\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Lag (k time steps)\")\n",
    "plt.ylabel(\"G(k)\")\n",
    "plt.title(r'Discrete Autocorrelation G(k) with highlights')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "1. **Strong Linear Autocorrelation (up to Lag 84):**  \n",
    "   There is a clear linear component in the autocorrelation function, starting from the origin and extending up to lag 84 (which corresponds to approximately 4 months).  \n",
    "   **Interpretation:** This suggests that values from the past 4 months significantly influence tomorrow’s value.  \n",
    "   *(To strengthen this claim, a statistical test or literature reference would be helpful.)*\n",
    "\n",
    "2. **Cyclic Patterns Identified:**  \n",
    "   The autocorrelation function exhibits local maxima at lags around 156 (approximately half a year) and 297 (roughly one year).  \n",
    "   **Interpretation:** This indicates the presence of a cyclic or seasonal component in the time series with semi-annual and annual periodicity.  \n",
    "   *(Further validation through spectral analysis or referencing known seasonal behaviors in financial data is recommended.)*\n",
    "\n",
    "[comment]: https://en.wikipedia.org/wiki/Central_moment"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate the mean\n",
    "mean_of_stocks = df_detrending.mean(axis=1)\n",
    "\n",
    "# Plot the mean_of_stocks\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(mean_of_stocks.index, mean_of_stocks.values)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Mean Value\")\n",
    "plt.title(\"Mean of Detrended\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0, color='black', linestyle='-')# Add main horizontal line at y=0\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate autocorrelation using statsmodels\n",
    "acf_values = sm.tsa.stattools.acf(mean_of_stocks, nlags=len(mean_of_stocks) - 1,  missing='conservative')\n",
    "\n",
    "# Plot the autocorrelation\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.stem(range(len(acf_values)), acf_values, basefmt=\" \")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.title(\"Autocorrelation Function of mean of stocks\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate autocorrelation using statsmodels\n",
    "acf_values = sm.tsa.stattools.acf(trend, nlags=len(trend) - 1,  missing='conservative')\n",
    "\n",
    "# Plot the autocorrelation\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.stem(range(len(acf_values)), acf_values, basefmt=\" \")\n",
    "plt.xlabel(\"Lag\")\n",
    "plt.ylabel(\"Autocorrelation\")\n",
    "plt.title(\"Autocorrelation Function of the Trend\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "last_date_prices = df_detrending.iloc[-1]\n",
    "\n",
    "# Sort the prices\n",
    "sorted_prices = np.sort(last_date_prices)\n",
    "\n",
    "# Calculate the CDF\n",
    "n = len(sorted_prices)\n",
    "cdf_values = np.arange(start=1, stop=n+1) / n\n",
    "\n",
    "# Plot the empirical CDF\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.step(sorted_prices, cdf_values)\n",
    "plt.xlabel('Stock Prices at Last Date')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Empirical Cumulative Distribution Function')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ecdf\n",
    "\n",
    "sample = df_detrending.iloc[-1]\n",
    "\n",
    "# Calculate the ECDF\n",
    "res = ecdf(sample)\n",
    "\n",
    "# Plot the ECDF\n",
    "plt.figure(figsize=(20, 10))\n",
    "res.cdf.plot(label='ECDF of Stock Prices at Last Date')\n",
    "\n",
    "plt.xlabel('Stock Prices at Last Date')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Empirical Cumulative Distribution Function')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "------------------------\n",
    "#6. Try to use Langevin equation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Langevin_equation\n",
    "\n",
    "How?\n",
    "\n",
    "Some comment: \"Another comment is the cyclic nature of the data due to, e.g., option expiration, earnings reports, and a few other things. This limits a study based on Langevin equation, as the process is not continuous. A neural net model would be more appropriate and practical.\"\n",
    "\n",
    "#7. Try to use the $\\frac{dM_i}{dt} = γ(t, M_i) +A(t)*ξ(t)$ model.\n",
    "\n",
    "How? Why?\n",
    "\n",
    "#8. Try to use Martingale representation theorem\n",
    "\n",
    "https://en.wikipedia.org/wiki/Martingale_representation_theorem"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 9.  Try \"Black-Scholes model\"\n",
    "\n",
    "https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model\n",
    "\n",
    "## 9.1. Take a logarithm of price\n",
    "\n",
    "Comment: \"You should take a logarithm of price, to follow the established wisdom of Black-Scholes model. Since the model is used to price options and other derivatives, it governs the markets regardless  of it being \"true\".\n",
    "\n",
    "## 9.2. Check \"Geometric Brownian motion\"\n",
    "https://en.wikipedia.org/wiki/Geometric_Brownian_motion"
   ]
  }
 ]
}
